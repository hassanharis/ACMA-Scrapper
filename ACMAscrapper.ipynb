{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cffa92ae",
   "metadata": {},
   "source": [
    "# ACMA Scrapper\n",
    "##### Â©Haris Hassan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d1ce03",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf8f62f-c03f-463e-9bef-7f958f3037fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##======================##\n",
    "## ACMAscrapper V 6.1.2 ##\n",
    "##======================##\n",
    "##\n",
    "## Scrapper for Register of Radiocommunications Licences of Australian Communications and Media Authority\n",
    "# \n",
    "# Author Haris Hassan\n",
    "# Email hharis11@hotmail.com\n",
    "# linkedin https://www.linkedin.com/in/hassanharis/\n",
    "#\n",
    "##=============================================================================\n",
    "# Import libraries\n",
    "\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import cProfile, pstats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3cf0e",
   "metadata": {},
   "source": [
    "## User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f3e9a8-b1e3-465b-8625-eb5955d7fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Replace Site \n",
    "sitecode = 9892\n",
    "\n",
    "##### Filters\n",
    "TRANSMITTER_ONLY = False\n",
    "\n",
    "THIS_CLIENT_ONLY = False\n",
    "THIS_CLIENT_ONLY_NAME = ''\n",
    "\n",
    "IGNORE_CLIENT = False\n",
    "IGNORE_CLIENT_NAME = ''\n",
    "\n",
    "FREQUENCY_FILTER = False\n",
    "MIN_FREQ = 650\n",
    "MAX_FREQ = 3800\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab')\n",
    "    running_in_colab = True\n",
    "else:\n",
    "    google_sheet_url = False\n",
    "    running_in_colab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0963d3d",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c8fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random user agent from the list\n",
    "def get_random_user_agent():\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Linux; Android 12; SM-S906N Build/QP1A.190711.020; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/80.0.3987.119 Mobile Safari/537.36',\n",
    "        'Mozilla/5.0 (Linux; Android 10; SM-G996U Build/QP1A.190711.020; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Mobile Safari/537.36',\n",
    "        'Mozilla/5.0 (Linux; Android 10; SM-G980F Build/QP1A.190711.020; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/78.0.3904.96 Mobile Safari/537.36',\n",
    "        'Mozilla/5.0 (Linux; Android 9; SM-G973U Build/PPR1.180610.011) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Mobile Safari/537.36',\n",
    "        'Mozilla/5.0 (Linux; Android 8.0.0; SM-G960F Build/R16NW) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.84 Mobile Safari/537.36',\n",
    "        'Mozilla/5.0 (Linux; Android 12; Pixel 6 Build/SD1A.210817.023; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/94.0.4606.71 Mobile Safari/537.36',\n",
    "        'Mozilla/5.0 (Linux; Android 11; Pixel 5 Build/RQ3A.210805.001.A1; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/92.0.4515.159 Mobile Safari/537.36',\n",
    "        'Mozilla/5.0 (iPhone14,3; U; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/602.1.50 (KHTML, like Gecko) Version/10.0 Mobile/19A346 Safari/602.1',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.111 Safari/537.36',\n",
    "        'Mozilla/5.0 (Linux; Android 5.0.2; SAMSUNG SM-T550 Build/LRX22G) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/3.3 Chrome/38.0.2125.102 Safari/537.36',\n",
    "        'Mozilla/5.0 (Linux; Android 7.0; SM-T827R4 Build/NRD90M) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.116 Safari/537.36',\n",
    "        'Mozilla/5.0 (Linux; Android 7.0; Pixel C Build/NRD90M; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/52.0.2743.98 Safari/537.36',\n",
    "        'Mozilla/5.0 (Linux; Android 11; Lenovo YT-J706X) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36',\n",
    "        'Mozilla/5.0 (Linux; Android 12; SM-X906C Build/QP1A.190711.020; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/80.0.3987.119 Mobile Safari/537.36',        \n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246'\n",
    "    ]\n",
    "    return random.choice(user_agents)\n",
    "\n",
    "def initialize_acma_site(soup):\n",
    "    SiteDetailsTitle, SiteDetails = scrape_table(webpage = soup)\n",
    "    SiteDetailsDictionary = {SiteDetailsTitle[i]: SiteDetails[i] for i in range(len(SiteDetailsTitle))}\n",
    "    SiteDetailsDictionary['Location'] = ' '.join(SiteDetailsDictionary['Location'].split())\n",
    "    return SiteDetailsDictionary\n",
    "    \n",
    "\n",
    "# Scrape webpage from the given URL and return the parsed content\n",
    "def scrape_page(url):\n",
    "    try:\n",
    "        response = session.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return BeautifulSoup(response.text, 'lxml')\n",
    "        else:\n",
    "            print(f\"Error: Failed to fetch data from {url}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "#Filter the antennas based on type, client and frequency\n",
    "def filter_assignments(assignments, TRANSMITTER_ONLY, THIS_CLIENT_ONLY, \n",
    "                       THIS_CLIENT_ONLY_NAME, IGNORE_CLIENT, IGNORE_CLIENT_NAME,\n",
    "                      FREQUENCY_FILTER, MAX_FREQ, MIN_FREQ):\n",
    "    assignments['Frequency (MHz)'] = frequency_to_mhz(assignments['Frequency'])\n",
    "    filteredAssignments = assignments\n",
    "    if TRANSMITTER_ONLY:\n",
    "        filteredAssignments = filteredAssignments.loc[filteredAssignments['T/R'] == 'T']\n",
    "    if THIS_CLIENT_ONLY:\n",
    "        filteredAssignments = filteredAssignments.loc[filteredAssignments['Client'] == THIS_CLIENT_ONLY_NAME]\n",
    "    if IGNORE_CLIENT:\n",
    "        filteredAssignments = filteredAssignments.loc[filteredAssignments['Client'] != IGNORE_CLIENT_NAME]\n",
    "    if FREQUENCY_FILTER:\n",
    "        filteredAssignments = filteredAssignments.loc[(filteredAssignments['Frequency (MHz)'] < MIN_FREQ) | (filteredAssignments['Frequency (MHz)']>MAX_FREQ)]\n",
    "    return filteredAssignments\n",
    "\n",
    "#Convert all frequencies to MHz\n",
    "def frequency_to_mhz(FREQ):\n",
    "    frequency = []\n",
    "    for x in FREQ:\n",
    "        if 'GHz' in x:\n",
    "            frequency.append(float(re.sub(' GHz','',x))*1000)\n",
    "        elif 'MHz' in x:\n",
    "            frequency.append(float(re.sub(' MHz','',x)))\n",
    "    return frequency\n",
    "\n",
    "def get_all_assignments_at_this_Site(soup):\n",
    "    assignments = soup.find('table',{\"class\": \"tablelist responsive\"})\n",
    "    assignments_headers = [td.text.strip() for td in assignments.select('th')]\n",
    "    assignments_data_main = scrape_assignments_pages(pd.DataFrame(columns = assignments_headers), soup)\n",
    "    assignments_data_main = assignments_data_main.reset_index(drop=True)\n",
    "    return assignments_data_main\n",
    "\n",
    "#Scrape all assignment entries in ACMA site assignment table\n",
    "def scrape_assignments_pages(assignments_data_main, assignments_page):\n",
    "    if assignments_page:\n",
    "        Assignments = assignments_page.find('table',{\"class\": \"tablelist responsive\"})\n",
    "        assignments_headers = [td.text.strip() for td in Assignments.select('th')]\n",
    "        assignments_data = pd.DataFrame(columns = assignments_headers)\n",
    "\n",
    "        assignment_links = []\n",
    "        for j in Assignments.find_all('tr')[1:]:\n",
    "            assignments_values = [tv.text.strip() for tv in j.find_all('td')]\n",
    "            assignments_data.loc[len(assignments_data)] = assignments_values\n",
    "            assignment_links.append([tl.get('href') for tl in j.find_all('a')][0])\n",
    "        assignments_data.insert(1,'links', assignment_links)\n",
    "        assignments_data_main = pd.concat([assignments_data_main, assignments_data], axis=0)\n",
    "\n",
    "        #Check if there's another Page\n",
    "        NEXT_PAGE_LINK = ''.join(['https://web.acma.gov.au' + x for x in  [tl.get('href') for tl in assignments_page.findAll('a',{'title':\"Next Page\"})]])\n",
    "\n",
    "        if NEXT_PAGE_LINK and NEXT_PAGE_LINK.strip():\n",
    "            print(NEXT_PAGE_LINK)\n",
    "            NEXT_PAGE = scrape_page(NEXT_PAGE_LINK)\n",
    "            return scrape_assignments_pages(assignments_data_main, NEXT_PAGE)\n",
    "        else:\n",
    "            return assignments_data_main\n",
    "    \n",
    "def start_Session():\n",
    "    \"\"\"\n",
    "    Start the session reusing the underlying TCP connection for multiple requests as\n",
    "    well as with gzip compression and randomly selected useragent \n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    session.headers.update({'Accept-Encoding': 'gzip'})\n",
    "    session.headers.update({'User-Agent': get_random_user_agent()})\n",
    "    return session\n",
    "\n",
    "def scrape_table(webpage, instance = 1):\n",
    "    \"\"\"\n",
    "    return the first column of the table as header list and 2nd column of table as values list\n",
    "    \"\"\"\n",
    "    table = webpage.select_one('table:nth-of-type('+str(instance)+')',{\"class\": \"tabledetail\"})\n",
    "    table_headers = [td.text for td in table.select('td:nth-of-type(1)') if not td.has_attr('colspan')]\n",
    "    table_Values = [td.text.strip() for td in table.select('td:nth-of-type(2)')]   \n",
    "    return table_headers, table_Values\n",
    "\n",
    "def find_link_destination(LinkedAssignmentsTable):\n",
    "    \"\"\"\n",
    "    Checking if Antenna information page has destination table and return destination site name if it has\n",
    "    otherwise return n/a\n",
    "    \"\"\"\n",
    "    LinkToList = []    \n",
    "    if LinkedAssignmentsTable:\n",
    "        for j in LinkedAssignmentsTable.find_all('tr')[1:]:\n",
    "            LinkToList.append([tv.text.strip() for tv in j.select('td:nth-of-type(5)')] )\n",
    "            \n",
    "    if LinkToList:\n",
    "        LinkToListTemp = list(dict.fromkeys([''.join(p) for p in LinkToList]))\n",
    "        for todel in LinkToListTemp:\n",
    "            if SiteDetailsDictionary['Location'] in todel:\n",
    "                LinkToListTemp.remove(todel)\n",
    "        LinkToList = ''.join(LinkToListTemp[:])\n",
    "        \n",
    "    if not LinkToList:\n",
    "        LinkToList = 'N/A'\n",
    "    return LinkToList\n",
    "\n",
    "#Scrape each antenna and add to antenna dataframe.\n",
    "def add_antenna_to_table(antennas_data, websoup, table, LinkedAssignmentsTable):\n",
    "    Antennaheaders, AntennaValues = scrape_table(websoup, table)\n",
    "    Antennaheaders.extend(['Destination Link'])\n",
    "    AntennaValues.extend([find_link_destination(LinkedAssignmentsTable)])\n",
    "    antennas_data_toAdd = pd.DataFrame([{Antennaheaders[i]: AntennaValues[i] for i in range(len(Antennaheaders))}])\n",
    "    antennas_data = pd.concat([antennas_data, antennas_data_toAdd], ignore_index=True)\n",
    "    return antennas_data\n",
    "\n",
    "def reformat_antennas_data(antennas_data):\n",
    "    antennas_data = antennas_data.fillna('')\n",
    "    if TRANSMITTER_ONLY:\n",
    "        antennas_data = antennas_data.drop(antennas_data[antennas_data['Device Type'] == 'Receiver'].index)\n",
    "    try:\n",
    "        antennas_data['Antenna']=([''.join(x[2].strip().title() +' '+ x[1].strip() + ' '+ x[0].strip().title()) for x in antennas_data['Antenna'].str.split(',', 2)])\n",
    "        antennas_data['Date Authorised']=([''.join(x[2] +'-'+ x[1] + '-'+ x[0].title()) for x in antennas_data['Date Authorised'].str.split('/', 2)])\n",
    "    except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "    if 'EFL ID' in antennas_data:\n",
    "        antennas_data['Device Registration ID'] = antennas_data['Device Registration ID'].astype(str) + antennas_data['EFL ID']\n",
    "  \n",
    "    antennas_data['Antenna']= antennas_data['Antenna'].str.replace('Rf Industries', 'RFI')\n",
    "    antennas_data['Antenna']= antennas_data['Antenna'].str.replace('Australia', '')\n",
    "    antennas_data['Antenna']= antennas_data['Antenna'].str.replace('Parallel Array Of Vertical Dipoles', 'Dipole Array')\n",
    "    antennas_data['Antenna']= antennas_data['Antenna'].str.replace('High Performance', '')\n",
    "    antennas_data['Antenna']= antennas_data['Antenna'].str.replace(' (Horizontal Polarisation)-Y', '', regex=False)\n",
    "    antennas_data['Antenna']= antennas_data['Antenna'].str.replace(' (Vertical Polarisation)-Y', '', regex=False)\n",
    "    \n",
    "\n",
    "    #antennas_data['Client']=antennas_data['Client'].apply(lambda x: x.title())\n",
    "    antennas_data['Client']=antennas_data['Client'].str.replace('Limited', 'Ltd')\n",
    "    antennas_data['Client']=antennas_data['Client'].str.replace('Ltd', '')\n",
    "    antennas_data['Client']=antennas_data['Client'].str.replace('Pty', '')\n",
    "    antennas_data['Client']=antennas_data['Client'].str.replace('Australia', '')\n",
    "    antennas_data['Client']=antennas_data['Client'].str.replace('NEW SOUTH WALES', 'NSW')\n",
    "    antennas_data['Client']=antennas_data['Client'].str.replace('GOVERNMENT', 'Government')\n",
    "    antennas_data['Client']=antennas_data['Client'].str.replace('Government TELECOMMUNICATIONS AUTHORITY', 'Telco Authority')\n",
    "\n",
    "    antennas_data['Date Authorised']=antennas_data['Date Authorised'].str.replace('Jan', '01')\n",
    "    antennas_data['Date Authorised']=antennas_data['Date Authorised'].str.replace('Feb', '02')\n",
    "    antennas_data['Date Authorised']=antennas_data['Date Authorised'].str.replace('Mar', '03')\n",
    "    antennas_data['Date Authorised']=antennas_data['Date Authorised'].str.replace('Apr', '04')\n",
    "    antennas_data['Date Authorised']=antennas_data['Date Authorised'].str.replace('May', '05')\n",
    "    antennas_data['Date Authorised']=antennas_data['Date Authorised'].str.replace('Jun', '06')\n",
    "    antennas_data['Date Authorised']=antennas_data['Date Authorised'].str.replace('Jul', '07')\n",
    "    antennas_data['Date Authorised']=antennas_data['Date Authorised'].str.replace('Aug', '08')\n",
    "    antennas_data['Date Authorised']=antennas_data['Date Authorised'].str.replace('Sep', '09')\n",
    "    antennas_data['Date Authorised']=antennas_data['Date Authorised'].str.replace('Oct', '10')\n",
    "    antennas_data['Date Authorised']=antennas_data['Date Authorised'].str.replace('Nov', '11')\n",
    "    antennas_data['Date Authorised']=antennas_data['Date Authorised'].str.replace('Dec', '12')\n",
    "\n",
    "    antennas_data['Antenna Polarisation']=antennas_data['Antenna Polarisation'].str.replace('Linear', '')\n",
    "    antennas_data['Transmitter Power']=antennas_data['Transmitter Power'].str.replace(' pY', '')\n",
    "    antennas_data['Transmitter Power']=antennas_data['Transmitter Power'].str.replace(' Mean Power', '')\n",
    "    antennas_data['Freq (MHz)'] = frequency_to_mhz(antennas_data['Emission Center Frequency'])\n",
    "\n",
    "    antennas_data['Antenna']= antennas_data['Antenna'].str.strip()\n",
    "    antennas_data['Antenna Polarisation']= antennas_data['Antenna Polarisation'].str.strip()\n",
    "    return antennas_data\n",
    "\n",
    "def scrape_antennas_data(assignments_links):\n",
    "    antennas_data = pd.DataFrame(columns = antennas_data_header)\n",
    "    NotFoundLinks = []        \n",
    "    for index, acmalink in enumerate(assignments_links):\n",
    "        st = time.time()\n",
    "        try:\n",
    "            soup2 = scrape_page('https://web.acma.gov.au' + acmalink)\n",
    "            linked_assignments_table = soup2.find(\"table\", {\"class\": \"tablelist linked-responsive\"})\n",
    "\n",
    "            try:\n",
    "                antennas_data = add_antenna_to_table(antennas_data, soup2, 1, linked_assignments_table)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "                print(acmalink)\n",
    "                continue\n",
    "\n",
    "            if acmalink[-2:]=='/1':\n",
    "                try:\n",
    "                    antennas_data = add_antenna_to_table(antennas_data, soup2, 2, linked_assignments_table)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(acmalink)\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('\\nException')\n",
    "            print(acmalink)\n",
    "            print('\\n')\n",
    "            NotFoundLinks.append(acmalink)\n",
    "            continue\n",
    "\n",
    "        et = time.time()\n",
    "        elapsed_time = et - st\n",
    "        print(str(index+1)+ '. ' + acmalink)\n",
    "        print('Execution time:', elapsed_time, 'seconds')\n",
    "    antennas_data = reformat_antennas_data(antennas_data)\n",
    "    display(NotFoundLinks)\n",
    "    return antennas_data, NotFoundLinks\n",
    "\n",
    "#Save the scraped and filtered results to a file with the given filename.\n",
    "def save_results_to_excel(antennas_data_export, filename):\n",
    "    Renamed_headers = ['Device ID', 'Antenna', 'Client','Type','Freq (MHz)','Power','Height', \n",
    "                      'Polarisation','Azimuth', 'Tilt','Licence','Date Authorised','Destination Link']\n",
    "    antennas_data_export = antennas_data_export.rename(columns={'Device Registration ID': 'Device ID', 'Device Type': 'Type', \n",
    "                                                          'Emission Center Frequency': 'Frequency'\n",
    "                                                          ,'Transmitter Power': 'Power'\n",
    "                                                          , 'Antenna Height (AGL)': 'Height', \n",
    "                                                          'Antenna Polarisation': 'Polarisation', \n",
    "                                                          'Antenna Azimuth': 'Azimuth',\n",
    "                                                          'Licence Number': 'Licence','Antenna Tilt': 'Tilt'})\n",
    "    antennas_data_export.sort_values(by=['Client', 'Antenna', 'Azimuth'], inplace=True)\n",
    "    antennas_data_export = antennas_data_export.reset_index(drop=True)\n",
    "    antennas_data_export.index += 1\n",
    "    antennas_data_export.to_excel(filename, columns = Renamed_headers, index=True)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "    antennas_data_export.to_html(r'C:\\Users\\Mewtwo\\Desktop\\Antennadata.html', columns = Renamed_headers, index=True)\n",
    "    if running_in_colab:\n",
    "        download_file_from_google_colab(filename)\n",
    "    return antennas_data_export[Renamed_headers]\n",
    "\n",
    "def download_file_from_google_colab(output_file_path):\n",
    "    from google.colab import files\n",
    "    files.download(output_file_path)\n",
    "    print(f\"File downloaded: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaa6996",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca65b8-cd0c-485e-b1e4-574302ab9c62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "    \n",
    "    ACMAsite_url = 'https://web.acma.gov.au/rrl/site_search.site_lookup?pSITE_ID=' + str(sitecode)\n",
    "\n",
    "    session = start_Session()\n",
    "    soup = scrape_page(ACMAsite_url)\n",
    "    SiteDetailsDictionary = initialize_acma_site(soup)\n",
    "    \n",
    "    file_name = 'ACMA ' + str(sitecode) + \" \" + re.sub(\"[\\\\\\\\/]\", \" \", SiteDetailsDictionary['Location']) + '.xlsx'\n",
    "    file_path = r'C:\\Users\\Mewtwo\\Desktop\\\\' + file_name\n",
    "    if running_in_colab:\n",
    "        file_path = file_name\n",
    "    antennas_data_header = ['Device Registration ID', 'Antenna', 'Client','Device Type','Emission Center Frequency',\n",
    "                          'Transmitter Power', 'Antenna Height (AGL)','Antenna Polarisation','Antenna Azimuth', \n",
    "                          'Antenna Tilt','Licence Number','Date Authorised','Destination Link']\n",
    "    \n",
    "    site_assignments = get_all_assignments_at_this_Site(soup)\n",
    "    site_assignments_filtered = filter_assignments(site_assignments, TRANSMITTER_ONLY, THIS_CLIENT_ONLY, THIS_CLIENT_ONLY_NAME, IGNORE_CLIENT, IGNORE_CLIENT_NAME,FREQUENCY_FILTER, MAX_FREQ, MIN_FREQ)\n",
    "    assignments_links = list(dict.fromkeys(site_assignments_filtered['links']) )\n",
    "    display('Found ' +str(len(site_assignments['links'])) + ' assignments and filtered ' + str(len(site_assignments_filtered['links'])) )\n",
    "\n",
    "    antennas_data, NotFoundLinks = scrape_antennas_data(assignments_links)\n",
    "    profiler.disable()\n",
    "    \n",
    "    save_results_to_excel(antennas_data, file_path)\n",
    "\n",
    "    stats = pstats.Stats(profiler).sort_stats('ncalls')\n",
    "    #stats.print_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2307ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
